inproceedings{03Bernuy22,
author = {Zavaleta Bernuy, Angela and Han, Ziwen and Shaikh, Hammad and Zheng, Qi Yin and Lim, Lisa-Angelique and Rafferty, Anna and Petersen, Andrew and Williams, Joseph Jay},
title = {How Can Email Interventions Increase Students’ Completion of Online Homework? A Case Study Using A/B Comparisons},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506874},
doi = {10.1145/3506860.3506874},
abstract = {Email communication between instructors and students is ubiquitous, and it could be valuable to explore ways of testing out how to make email messages more impactful. This paper explores the design space of using emails to get students to plan and reflect on starting weekly homework earlier. We deployed a series of email reminders using randomized A/B comparisons to test alternative factors in the design of these emails, providing examples of an experimental paradigm and metrics for a broader range of interventions. We also surveyed and interviewed instructors and students to compare their predictions about the effectiveness of the reminders with their actual impact. We present our results on which seemingly obvious predictions about effective emails are not borne out, despite there being evidence for further exploring these interventions, as they can sometimes motivate students to attempt their homework more often. We also present qualitative evidence about student opinions and behaviours after receiving the emails, to guide further interventions. These findings provide insight into how to use randomized A/B comparisons in everyday channels such as emails, to provide empirical evidence to test our beliefs about the effectiveness of alternative design choices. },
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {107–118},
numpages = {12},
keywords = {Randomized experiments, Reminder, Embedded experimentation, Procrastination, A/B comparisons},
location = {Online, USA},
series = {LAK22}
}

InProceedings{06Zavaleta21,
author={Angela Zavaleta-Bernuy, Qi Yin Zheng, Hammad Shaikh, Jacob Nogas, Anna Rafferty, Andrew Petersen, Joseph Jay Williams},
editor="Roll, Ido
and McNamara, Danielle
and Sosnovsky, Sergey
and Luckin, Rose
and Dimitrova, Vania",
title="Using Adaptive Experiments to Rapidly Help Students",
booktitle="Artificial Intelligence in Education",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="422--426",
abstract="Adaptive experiments can increase the chance that current students obtain better outcomes from a field experiment of an instructional intervention. In such experiments, the probability of assigning students to conditions changes while more data is being collected, so students can be assigned to interventions that are likely to perform better. Digital educational environments lower the barrier to conducting such adaptive experiments, but they are rarely applied in education. One reason might be that researchers have access to few real-world case studies that illustrate the advantages and disadvantages of these experiments in a specific context. We evaluate the effect of homework email reminders in students by conducting an adaptive experiment using the Thompson Sampling algorithm and compare it to a traditional uniform random experiment. We present this as a case study on how to conduct such experiments, and we raise a range of open questions about the conditions under which adaptive randomized experiments may be more or less useful.",
isbn="978-3-030-78270-2",
url={https://link.springer.com/chapter/10.1007/978-3-030-78270-2_75},
doi={10.1007/978-3-030-78270-2_75}
}

@inproceedings{06Price21,
author = {Thomas W. Price and Samiha Marwan and Joseph Jay Williams},
title = {Exploring Design Choices in Data-Driven Hints for Python Programming Homework},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460159},
doi = {10.1145/3430895.3460159},
abstract = {Students often struggle during programming homework and may need help getting started or localizing errors. One promising and scalable solution is to provide automated programming hints, generated from prior student data, which suggest how a student can edit their code to get closer to a solution, but little work has explored how to design these hints for large-scale, real-world classroom settings, or evaluated such designs. In this paper, we present CodeChecker, a system which generates hints automatically using student data, and incorporates them into an existing CS1 online homework environment, used by over 1000 students per semester. We present insights from survey and interview data, about student and instructor perceptions of the system. Our results highlight affordances and limitations of automated hints, and suggest how specific design choices may have impacted their effectiveness.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {283–286},
numpages = {4},
keywords = {automated programming hints, computing education},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{06Reza21,
author = {Mohi Reza and Juho Kim and Ananya Bhattacharjee and Anna N. Rafferty and Joseph Jay Williams},
title = {The MOOClet Framework: Unifying Experimentation, Dynamic Improvement, and Personalization in Online Courses},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460128},
doi = {10.1145/3430895.3460128},
abstract = {How can educational platforms be instrumented to accelerate the use of research to improve students' experiences? We show how modular components of any educational interface - e.g. explanations, homework problems, even emails - can be implemented using the novel MOOClet software architecture. Researchers and instructors can use these augmented MOOClet components for: (1) Iterative Cycles of Randomized Experiments that test alternative versions of course content; (2) Data-Driven Improvement using adaptive experiments that rapidly use data to give better versions of content to future students, on the order of days rather than months. A MOOClet supports both manual and automated improvement using reinforcement learning; (3) Personalization by delivering alternative versions as a function of data about a student's characteristics or subgroup, using both expert-authored rules and data mining algorithms. We provide an open-source web service for implementing MOOClets (www.mooclet.org) that has been used with thousands of students. The MOOClet framework provides an ecosystem that transforms online course components into collaborative micro-laboratories, where instructors, experimental researchers, and data mining/machine learning researchers can engage in perpetual cycles of experimentation, improvement, and personalization.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {15–26},
numpages = {12},
keywords = {A/B comparisons, randomized experiments, massive open online courses, dynamic improvement, multi-armed bandits, education technology, personalization},
location = {Virtual Event, Germany},
series = {L@S '21}
}

inproceedings{03Petersen21-2,
author = {Angela Zavaleta Bernuy and Qi Yin Zheng and Hammad Shaikh and Andrew Petersen and Joseph Jay Williams},
title = {Investigating the Impact of Online Homework Reminders Using Randomized A/B Comparisons},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432427},
doi = {10.1145/3408877.3432427},
abstract = {Procrastination by students may lead to adverse outcomes such as a focus on completion rather than learning or even a failure to complete learning tasks. One common method for motivating students and reducing procrastination is to send reminders with hints and study strategies, but it's not clear if these messages are effective or when is the best time to send them. Randomized A/B comparisons could be used to try different reminders or alternative ideas about how best to get students to start work earlier and, crucially, to measure the impact of these interventions on behaviour. This paper describes an A/B comparison of reminder emails set in a large CS1 course at a research-focused North American university. We found evidence that the email interventions caused a higher proportion of students to attempt the online homework but did not see evidence that these particular emails got students to start early, irrespective of changes to the timing of the reminder. More broadly, these findings illustrate how to use A/B comparisons in educational settings to test ideas about how to help students, and demonstrate the value of using randomized A/B comparisons, even when evaluating actions that seem obviously beneficial, such as reminder emails.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {921–927},
numpages = {7},
keywords = {A/B comparisons, procrastination, reminders, CS1, email},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

inproceedings{03Petersen21,
author = {Jaemarie Solyst and Trisha Thakur and Madhurima Dutta and Yuya Asano and Andrew Petersen and Joseph Jay Williams},
title = {Procrastination and Gaming in an Online Homework System of an Inverted CS1},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432440},
doi = {10.1145/3408877.3432440},
abstract = {Engaged preparation and study in combination with lectures are important for all courses but are particularly critical for online, hybrid, and inverted classrooms. Many instructors use online systems to deliver new course content and exercises, but students often delay assignments or game these systems (e.g., guessing on multiple-choice questions), often to the detriment of their learning. In an inverted CS1 course, many students self-reported high rates of gaming-the-system behavior, so we examine survey data to identify factors that contribute to engagement in these maladaptive behaviours. We supplement that analysis with interview data to gain a deeper understanding of the situation. We also implemented and evaluated a previously reported online intervention aimed at reducing gaming behavior. Unlike prior work, our intervention did not have a significant effect on guessing behavior. We discuss why the factors we identified might explain this result, as well as suggest future work to improve our understanding of gaming behaviours and inform the design of systems that encourage effective learning.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {789–795},
numpages = {7},
keywords = {online learning, inverted classroom, gaming the system, procrastination},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Marwan19,
 author = {Samiha Marwan and Joseph Jay Williams and Thomas Price},
 title = {An Evaluation of the Impact of Automated Programming Hints on Performance and Learning},
 booktitle = {Proceedings of the 2019 ACM Conference on International Computing Education Research},
 series = {ICER '19},
 year = {2019},
 isbn = {978-1-4503-6185-9},
 location = {Toronto ON, Canada},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3291279.3339420},
 doi = {10.1145/3291279.3339420},
 acmid = {3339420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {block-based programming, computer science education, next-step hints, self-explanation},
} 

@inproceedings{04Williams18,
 author = {Joseph Jay Williams and Anna N. Rafferty and Dustin Tingley and Andrew Ang and Walter S. Lasecki and Juho Kim},
 title = {Enhancing Online Problems Through Instructor-Centered Tools for Randomized Experiments},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {207:1--207:12},
 articleno = {207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3173574.3173781},
 doi = {10.1145/3173574.3173781},
 acmid = {3173781},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic experimentation, instructional design, online education, randomized experiments},
}

inproceedings{05Zhang19,
 author = {Lisa Zhang and Michelle Craig and Mark Kazakevich and Joseph Jay Williams},
 title = {Experience Report: Mini Guest Lectures in a CS1 Course via Video Conferencing},
 booktitle = {Proceedings of the 1st ACM Global Computing Education Conference},
 year = {2019},
 comment = {To appear.}
}
