@inproceedings{06Price21,
author = {Thomas W. Price and Samiha Marwan and Joseph Jay Williams},
title = {Exploring Design Choices in Data-Driven Hints for Python Programming Homework},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460159},
doi = {10.1145/3430895.3460159},
abstract = {Students often struggle during programming homework and may need help getting started or localizing errors. One promising and scalable solution is to provide automated programming hints, generated from prior student data, which suggest how a student can edit their code to get closer to a solution, but little work has explored how to design these hints for large-scale, real-world classroom settings, or evaluated such designs. In this paper, we present CodeChecker, a system which generates hints automatically using student data, and incorporates them into an existing CS1 online homework environment, used by over 1000 students per semester. We present insights from survey and interview data, about student and instructor perceptions of the system. Our results highlight affordances and limitations of automated hints, and suggest how specific design choices may have impacted their effectiveness.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {283–286},
numpages = {4},
keywords = {automated programming hints, computing education},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{06Reza21,
author = {Mohi Reza and Juho Kim and Ananya Bhattacharjee and Anna N. Rafferty and Joseph Jay Williams},
title = {The MOOClet Framework: Unifying Experimentation, Dynamic Improvement, and Personalization in Online Courses},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460128},
doi = {10.1145/3430895.3460128},
abstract = {How can educational platforms be instrumented to accelerate the use of research to improve students' experiences? We show how modular components of any educational interface - e.g. explanations, homework problems, even emails - can be implemented using the novel MOOClet software architecture. Researchers and instructors can use these augmented MOOClet components for: (1) Iterative Cycles of Randomized Experiments that test alternative versions of course content; (2) Data-Driven Improvement using adaptive experiments that rapidly use data to give better versions of content to future students, on the order of days rather than months. A MOOClet supports both manual and automated improvement using reinforcement learning; (3) Personalization by delivering alternative versions as a function of data about a student's characteristics or subgroup, using both expert-authored rules and data mining algorithms. We provide an open-source web service for implementing MOOClets (www.mooclet.org) that has been used with thousands of students. The MOOClet framework provides an ecosystem that transforms online course components into collaborative micro-laboratories, where instructors, experimental researchers, and data mining/machine learning researchers can engage in perpetual cycles of experimentation, improvement, and personalization.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {15–26},
numpages = {12},
keywords = {A/B comparisons, randomized experiments, massive open online courses, dynamic improvement, multi-armed bandits, education technology, personalization},
location = {Virtual Event, Germany},
series = {L@S '21}
}

inproceedings{06Asano21,
author = {Yuya Asano and Madhurima Dutta and Trisha Thakur and Jaemarie Solyst and Stephanie Cristea and Helena Jovic and Andrew Petersen and Joseph Jay Williams},
title = {Exploring Additional Personalized Support While Attempting Exercise Problems in Online Learning Platforms},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460145},
doi = {10.1145/3430895.3460145},
abstract = {In online asynchronous learning environments, students are assigned exercises, but it is not clear how to incorporate the kinds of actions an in-person tutor might take such as explaining, providing more practice, prompting for reflection, and motivating. We explore approaches to adding "Drop-Downs'' that appear after a student submits an answer and that contain additional information to support learning. We conducted randomized A/B experiments exploring the impact of these Drop-Downs on student learning in the online portion of a flipped CS1 course. The deployed Drop-Downs in this course provided explanations, reflective prompts, additional problems, and motivational messages. The results suggest that students benefit from various Drop-Downs in different contexts, indicating the possibility of personalizing content based on the student's state. We discuss the resulting design implications of Drop-Downs in online learning systems.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {235–238},
numpages = {4},
keywords = {field deployment, online learning, A/B testing, intervention, randomized experiments, personalization},
location = {Virtual Event, Germany},
series = {L@S '21}
}

inproceedings{03Petersen21-2,
author = {Angela Zavaleta Bernuy and Qi Yin Zheng and Hammad Shaikh and Andrew Petersen and Joseph Jay Williams},
title = {Investigating the Impact of Online Homework Reminders Using Randomized A/B Comparisons},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432427},
doi = {10.1145/3408877.3432427},
abstract = {Procrastination by students may lead to adverse outcomes such as a focus on completion rather than learning or even a failure to complete learning tasks. One common method for motivating students and reducing procrastination is to send reminders with hints and study strategies, but it's not clear if these messages are effective or when is the best time to send them. Randomized A/B comparisons could be used to try different reminders or alternative ideas about how best to get students to start work earlier and, crucially, to measure the impact of these interventions on behaviour. This paper describes an A/B comparison of reminder emails set in a large CS1 course at a research-focused North American university. We found evidence that the email interventions caused a higher proportion of students to attempt the online homework but did not see evidence that these particular emails got students to start early, irrespective of changes to the timing of the reminder. More broadly, these findings illustrate how to use A/B comparisons in educational settings to test ideas about how to help students, and demonstrate the value of using randomized A/B comparisons, even when evaluating actions that seem obviously beneficial, such as reminder emails.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {921–927},
numpages = {7},
keywords = {A/B comparisons, procrastination, reminders, CS1, email},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

inproceedings{03Petersen21,
author = {Jaemarie Solyst and Trisha Thakur and Madhurima Dutta and Yuya Asano and Andrew Petersen and Joseph Jay Williams},
title = {Procrastination and Gaming in an Online Homework System of an Inverted CS1},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432440},
doi = {10.1145/3408877.3432440},
abstract = {Engaged preparation and study in combination with lectures are important for all courses but are particularly critical for online, hybrid, and inverted classrooms. Many instructors use online systems to deliver new course content and exercises, but students often delay assignments or game these systems (e.g., guessing on multiple-choice questions), often to the detriment of their learning. In an inverted CS1 course, many students self-reported high rates of gaming-the-system behavior, so we examine survey data to identify factors that contribute to engagement in these maladaptive behaviours. We supplement that analysis with interview data to gain a deeper understanding of the situation. We also implemented and evaluated a previously reported online intervention aimed at reducing gaming behavior. Unlike prior work, our intervention did not have a significant effect on guessing behavior. We discuss why the factors we identified might explain this result, as well as suggest future work to improve our understanding of gaming behaviours and inform the design of systems that encourage effective learning.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {789–795},
numpages = {7},
keywords = {online learning, inverted classroom, gaming the system, procrastination},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Marwan19,
 author = {Samiha Marwan and Joseph Jay Williams and Thomas Price},
 title = {An Evaluation of the Impact of Automated Programming Hints on Performance and Learning},
 booktitle = {Proceedings of the 2019 ACM Conference on International Computing Education Research},
 series = {ICER '19},
 year = {2019},
 isbn = {978-1-4503-6185-9},
 location = {Toronto ON, Canada},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3291279.3339420},
 doi = {10.1145/3291279.3339420},
 acmid = {3339420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {block-based programming, computer science education, next-step hints, self-explanation},
} 

@inproceedings{04Williams18,
 author = {Joseph Jay Williams and Anna N. Rafferty and Dustin Tingley and Andrew Ang and Walter S. Lasecki and Juho Kim},
 title = {Enhancing Online Problems Through Instructor-Centered Tools for Randomized Experiments},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {207:1--207:12},
 articleno = {207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3173574.3173781},
 doi = {10.1145/3173574.3173781},
 acmid = {3173781},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic experimentation, instructional design, online education, randomized experiments},
}

inproceedings{05Zhang19,
 author = {Lisa Zhang and Michelle Craig and Mark Kazakevich and Joseph Jay Williams},
 title = {Experience Report: Mini Guest Lectures in a CS1 Course via Video Conferencing},
 booktitle = {Proceedings of the 1st ACM Global Computing Education Conference},
 year = {2019},
 comment = {To appear.}
}
