@inproceedings{03Bui23,
author = {Giang Bui and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Andrew Petersen},
title = {Prior Programming Experience: A Persistent Performance Gap in CS1 and CS2},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569752},
doi = {10.1145/3545945.3569752},
abstract = {Previous work has reported on the advantageous effects of prior experience in CS1, but it remains unclear whether these effects fade over a sequence of introductory programming courses. Furthermore, while student perceptions suggest that prior experience remains important, studies have reported that a student's expectation of their performance is a more accurate predictor of outcome. We aim to confirm if prior experience (formal or informal) provides short-term and long-term advantages in computing courses or if the advantage fades. Furthermore, we explore whether the expectation of performance is a more accurate predictor of student success than informal and formal prior experience. To explore these questions, we deployed surveys in a CS1 course to gauge students' level of prior experience in programming, prediction of final exam grades, and self-efficacy to succeed in university. Grades from CS1 and CS2 were also collected. We observed a persistent (1-letter grade) gap between the performance of students with no prior experience and those with any experience, but we did not observe a noteworthy gap when comparing student performance based on formal or informal experience. We also observed differences in self-efficacy and retention rates between different levels of prior experience. Lastly, we confirm that success in CS1 can be better reflected and predicted by some controllable factors, such as students' perceptions of ability.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {889–895},
numpages = {7},
keywords = {cs2, prior experience, self-efficacy, confidence, cs1, prediction},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{06Liut22,
author = {Naaz Sibia and Michael Liut},
title = {The Positive Effects of Using Reflective Prompts in a Database Course},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535323},
doi = {10.1145/3531072.3535323},
abstract = {Motivation: Prior literature has identified student reflections as a way to encourage students to express their thoughts in a structured and focused manner. Objectives: Our goal is to examine the impact of reflections in a third year database systems course, which employs an active learning approach and classroom environment. Specifically, we are interested in seeing whether reflecting on key concepts covered in a preparatory component before lecture had an impact on student’s immediate and long-term performance. Methods: Students were divided into two groups, and asked to reflect on different topics after watching lecture videos before completing their homework exercises for 3 weeks. Results: We observed that students who reflected on lecture concepts performed better on homework exercises that covered those same concepts than students who did not reflect on those same concepts. Moreover, students who reflected performed better in subsequent assessments than students who did not reflect at all. Implications: Reflection as a part of the preparatory component in flipped classrooms is a useful component in conceptual understanding. Further research and investigation should be pursued into ways of prompting reflection, and assessing this component in database courses.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {32–37},
numpages = {6},
keywords = {Active Learning, Long-Term Memory, Computer Science Education, Reflective Prompts, Databases, Student Performance},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

inproceedings{03Harrington22,
author = {Angela Zavaleta Bernuy and Anna Ly and Brian Harrington and Michael Liut and Andrew Petersen and Sadia Sharmin and Lisa Zhang},
title = {Additional Evidence for the Prevalence of the Impostor Phenomenon in Computing},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499282},
doi = {10.1145/3478431.3499282},
abstract = {Motivation Despite the widespread belief that computing practitioners frequently experience the Imposter Phenomenon (IP), little formal work has measured the prevalence of IP in the computing community despite its negative effect on achievement.Objectives This study aims to replicate recent work that has suggested that IP experiences are widespread in computing students and to extend that work by exploring the relationship between the IP, progress in the program, and ethnic identity.Methods A survey with several demographic questions (gender, ethnicity, international status, and year of study) and Clance's IP scale (CIPS) was deployed to students in post-secondary computing courses. Correlations between demographic factors and CIPS scores were evaluated, and a linear model was constructed to explore the interaction between demographic factors of interest.Results We reaffirm that a high proportion of CS students meet the IP diagnostic criteria and that women report higher CIPS scores than men. We also present evidence that Asian students with domestic and international status report different levels of IP experiences.Discussion These findings highlight the importance – to educators at all levels – of cultivating belonging in computing communities.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education},
pages = {654–660},
numpages = {7},
keywords = {imposter syndrome, impostor phenomenon, belonging},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{03Liut22,
author = {Pan Chen and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Joseph Jay Williams},
title = {Investigating the Impact of Voice Response Options in Surveys},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499087},
doi = {10.1145/3478432.3499087},
abstract = {With the widespread usage of mobile devices, users can now choose to provide input through voice or text. As researchers frequently ask students open-ended questions, we want to explore a natural mode to obtain better feedback in surveys. This study details a preliminary study demonstrating the importance of allowing students to choose between voice or text input to respond to surveys. A survey with several open-ended questions was deployed in a CS1 course. Correlations between the gender of the respondent and their method of responding were evaluated. We found that voice responses tended to be longer and preferred more by females relative to male students.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1124},
numpages = {1},
keywords = {surveys, open-ended questions, voice responses, cs1},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{11Liut20,
author = {Muyu Wang and Naaz Sibia and Ilir Dema and Michael Liut and Carlos An\'{\i}bal Su\'{a}rez},
title = {Building a Better SQL Automarker for Database Courses},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3489970},
doi = {10.1145/3488042.3489970},
abstract = {This work introduces and demonstrates the viability of a novel SQL automarking tool (“SQAM”) that: (1) provides a fair grade to the student, one which matches the student’s effort and understanding of the course material, and (2) to provide personalized feedback, allowing the student to remain engaged in the material and learn from their mistakes while still being in that headspace. Additionally, we strive to ensure that our tool maintains the same standards (grade and feedback) that a highly qualified member of teaching staff would produce, so we compare and contrast our automarker’s results to that of teaching assistants over several historic offerings of the same database course at a large research intensive public institution, while reducing the grading time, thus enabling the teaching staff to channel more time into instruction. Furthermore, we describe SQAM’s design and our model which applies the aggregate result of four different string similarity metrics to compute solution similarity in conjunction with our discretization process to fairly evaluate a student’s submission. Our results show that SQAM produces very similar grades to those which were historically given by teaching assistants. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {37},
numpages = {3},
keywords = {SQL Automarking, Partial Marking, String Regularities, String Similarity, Database Course Tools, Student Feedback Enhancement},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@inproceedings{11ly21,
author = {Anna Ly and Jack Parkinson and Quintin Cutts and Michael Liut and Andrew Petersen},
title = {Spatial Skills and Demographic Factors in CS1},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3488049},
doi = {10.1145/3488042.3488049},
abstract = {Motivation Prior studies have established that training spatial skills may improve outcomes in computing courses. Very few of these studies have, however, explored the impact of spatial skills training on women or examined its relationship with other factors commonly explored in the context of academic performance, such as socioeconomic background and self-efficacy. Objectives In this study, we report on a spatial skills intervention deployed in a computer programming course (CS1) in the first year of a post-secondary program. We explore the relationship between various demographic factors, course performance, and spatial skills ability at both the beginning and end of the term. Methods Data was collected using a combination of demographic surveys, existing self-efficacy and CS1 content instruments, and the Revised PVST:R spatial skills assessment. Spatial skills were evaluated both at the beginning of the term and at the end, after spatial skills training was provided. Results While little evidence was found to link spatial skills to socioeconomic status or self-efficacy, both gender identity and previous experience in computing were found to be correlated to spatial skills ability at the start of the course. Women initially recorded lower spatial skills ability, but after training, the distribution of spatial skills scores for women approached that of men. Discussion These findings suggest that, if offered early enough, spatial skills training may be able to remedy some differences in background that impact performance in computing courses. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {4},
numpages = {10},
keywords = {retention, spatial skills, socioeconomic status, CS1, gender},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@article{12parkinson21,
author = {Jack Parkinson and Ryan Bockmon and Quintin Cutts and Michael Liut and Andrew Petersen and Sheryl Sorby},
title = {Practice Report: Six Studies of Spatial Skills Training in Introductory Computer Science},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3494574},
doi = {10.1145/3494574},
journal = {ACM Inroads},
month = {nov},
pages = {18–29},
numpages = {12}
}

@inproceedings{10ly2021revisiting,
  title={Revisiting Syntax Exercises in CS1},
  author={Anna Ly and John Edwards and Michael Liut and Andrew Petersen},
  booktitle={Proceedings of the 22st Annual Conference on Information Technology Education},
  pages={9--14},
  year={2021},
  url = {https://doi.org/10.1145/3450329.3476855},
  doi = {10.1145/3450329.3476855},
}

inproceedings{03Zhang21,
author = {Larry Yueli Zhang and Andrew K. Petersen and Michael Liut and Bogdan Simion and Furkan Alaca},
title = {A Multi-Course Report on the Experience of Unplanned Online Exams},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432515},
doi = {10.1145/3408877.3432515},
abstract = {We report our experience of preparing and conducting unplanned online exams in the unique half-physical, half-virtual semester of Winter 2020. The report covers four courses in a large university's computer science program, ranging from first-year to third-year. With the data generated by students taking both in-person and online exams in multiple courses, we perform analyses to evaluate the validity of the online exams (especially the unproctored ones) as an assessment of student understanding. With the fine-grained student activity data provided by the online exam platform, we are also able to investigate the patterns in student exam-taking behaviours and their correlations with student performance on the exam. In addition, we share, in detail, the tips and lessons that were learned throughout the process of designing, implementing, and hosting the online exams.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {17–23},
numpages = {7},
keywords = {student behaviour, online exams, assessment},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Liut20,
 author = {Arnaud Deza and Haocheng Hu and Vaishvik Maisuria and Michael Liut and Andrew Petersen and Bogdan Simion},
 title = {Using Discussion Board Data to Hire Teaching Assistants},
 booktitle = {Proceedings of the 6th SPLICE Workshop at L@S},
 year = {2020},
 numpage = {6},
 url = {https://cssplice.github.io/LAS20/proc/SPLICE_2020_LS_paper_9.pdf},
}

@inproceedings{06Liut20,
author = {Simon and Oscar Karnalim and Judy Sheard and Ilir Dema and Amey Karkare and Juho Leinonen and Michael Liut and Ren\'{e}e McCauley},
title = {Choosing Code Segments to Exclude from Code Similarity Detection},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439201},
doi = {10.1145/3437800.3439201},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {1–19},
numpages = {19},
keywords = {collusion, academic integrity, plagiarism, code similarity detection},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}
